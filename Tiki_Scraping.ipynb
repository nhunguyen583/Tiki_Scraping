{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tiki_Scraping",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WEAlnXELotqm",
        "outputId": "e6899beb-3fef-408c-9e08-169ed89a6abd"
      },
      "source": [
        "!pip install selenium\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install webdriver-manager\n",
        "\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Set driver for Chrome\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')\n",
        "\n",
        "#get data of 1 page\n",
        "def selenium_scrap(tiki_url):\n",
        "  driver = webdriver.Chrome('chromedriver',options=options)        # Define the chrome drivers with setting options we define above\n",
        "  driver.implicitly_wait(20)                                       # We let selenium to wait for 30 seconds for all javascript script done before return the result of HTML\n",
        "\n",
        "  driver.get(tiki_url)                                             # Open the browser again to get web page\n",
        "  html_data = driver.page_source                                   # After driver.get() is done, you can get back HTML string by using .page_source\n",
        "  driver.close()                                                   # Close the driver after retrieving the web page\n",
        "\n",
        "  soup = BeautifulSoup(html_data, 'html.parser')                   # Do your beautifulsoup business like the usual\n",
        "\n",
        "  return soup\n",
        "\n",
        "#scrap 1 page\n",
        "def page_scrap(url):\n",
        "\n",
        "  data = []\n",
        "  soup = selenium_scrap(url)\n",
        "\n",
        "  products = soup.find_all('a', {'class':'product-item'})\n",
        "  script = None\n",
        "  scripts_list = []\n",
        "\n",
        "  # other solution to Stop the script, when we found string 'Rất tiếc, không ...' we will stop code\n",
        "  #\n",
        "  #   if soup.find('div', {'class':'VLxTK'}).text == 'Rất tiếc, không tìm thấy sản phẩm phù hợp với lựa chọn của bạn':\n",
        "  #     return data.append('Stop')\n",
        "  #   else:\n",
        "  #     products = soup.find_all('a', {'class':'product-item'})\n",
        "  #....\n",
        "\n",
        "\n",
        "  while True: #check if page with no product\n",
        "    if len(products) == 0:\n",
        "      data.append('Stop')\n",
        "      return data\n",
        "    else:\n",
        "      for ldjson in soup.find_all('script'):\n",
        "        if '\"@type\":\"Product\"' in ldjson.text:\n",
        "          script = json.loads(ldjson.text)\n",
        "          scripts_list.append(script)\n",
        "\n",
        "      i = 0\n",
        "      for product in products:\n",
        "\n",
        "        d = {'title':'' , 'price':'' , 'tikinow':'' , 'freeship':'' , 'reviews':'' , 'product_id':'' ,\n",
        "            'link':'' , 'rating_value':'' , 'image url':'',\n",
        "            'badge under price': '', 'discount percentage': '', 'installment':'', 'free gifts': ''}\n",
        "\n",
        "        #product name\n",
        "        d['title'] = product.find('div',{'class' : 'name'}).text\n",
        "\n",
        "        #product price\n",
        "        d['price'] = int(re.sub('[. ₫]','', product.find('div',{'class':'price-discount__price'}).text))\n",
        "\n",
        "        #product id / sku\n",
        "        d['product_id'] = scripts_list[i]['sku']\n",
        "\n",
        "        #product url\n",
        "        d['link'] = scripts_list[i]['url']\n",
        "\n",
        "        #discount percentage #Chinh: get only number of discount\n",
        "        try:\n",
        "          d['dis_percentage'] = int(re.sub('[-%]','', product.find('div',{'class':'price-discount__discount'}).text))\n",
        "        except:\n",
        "          d['dis_percentage'] = 0\n",
        "\n",
        "        #tikinow\n",
        "        try:\n",
        "          d['tikinow'] = product.find('div',{'class':'badge-service'}).img['src'].replace(\"https://salt.tikicdn.com/ts/upload/9f/32/dd/8a8d39d4453399569dfb3e80fe01de75.png\",\"Yes\")\n",
        "        except:\n",
        "          d['tikinow'] = 'No'\n",
        "\n",
        "        #freeship\n",
        "        try:\n",
        "          if product.find('div',{'class':'badge-top'}).text == 'Freeship':\n",
        "            d['freeship'] = 'Yes'\n",
        "          else:\n",
        "            d['freeship'] = 'No'\n",
        "        except:\n",
        "          d['freeship'] = 'No'\n",
        "\n",
        "        #reviews number\n",
        "        try:\n",
        "          d['reviews'] = int(re.sub('[()]','', product.find('div',{'class':'review'}).text))\n",
        "        except:\n",
        "          d['reviews'] = 0\n",
        "\n",
        "        #rating value\n",
        "        try:\n",
        "          d['rating_value'] = int(scripts_list[i]['aggregateRating']['ratingValue'])\n",
        "        except:\n",
        "          d['rating_value'] = 0\n",
        "\n",
        "        #Nhu part\n",
        "        #img url\n",
        "        try:\n",
        "          d['image url'] = product.img['src']\n",
        "        except:\n",
        "          d['image url'] = 'No'\n",
        "\n",
        "        # try:\n",
        "        #   d['page url'] = product['href']\n",
        "        # except:\n",
        "        #   d['page url'] = 'No'\n",
        "\n",
        "        #badge under price\n",
        "        if product.find('img',{'src':'https://salt.tikicdn.com/ts/upload/51/ac/cc/528e80fe3f464f910174e2fdf8887b6f.png'}):\n",
        "          d['badge under price'] = 'Yes'\n",
        "        else:\n",
        "          d['badge under price'] = 'No'\n",
        "\n",
        "        # Nhu: text discount percentage\n",
        "        # try:\n",
        "        #   d['discount percentage'] = product.find('div',{'class':'price-discount__discount'}).text[1:]\n",
        "        # except:\n",
        "        #   d['discount percentage'] = 0\n",
        "\n",
        "        #installment\n",
        "        if product.find('span', string = 'Trả góp'):\n",
        "          d['installment'] = 'Yes'\n",
        "        else:\n",
        "          d['installment'] = 'No'\n",
        "\n",
        "        #free gift\n",
        "        if product.find('div',{'class':'gift-image-list'}):\n",
        "          d['free gifts'] = 'Yes'\n",
        "        else:\n",
        "          d['free gifts'] = 'No'\n",
        "\n",
        "        i+=1\n",
        "        data.append(d)\n",
        "\n",
        "      return data\n",
        "\n",
        "#scrap all pages available\n",
        "def web_scrap():\n",
        "  data = ['Start'] #set flag Start to start for loop below\n",
        "  page = 1\n",
        "  for page in range(1,15): #we can use range(1, 1000000000 what ever you want)\n",
        "    tiki_urls = f'https://tiki.vn/dien-thoai-may-tinh-bang/c1789?page={page}&src=c.1789.hamburger_menu_fly_out_banner'\n",
        "    if data[-1] == 'Stop': #if the last element of data is 'Stop', we will stop script and return data\n",
        "      break\n",
        "    else:\n",
        "      data += page_scrap(tiki_urls)\n",
        "    page += 1\n",
        "  print(page)\n",
        "  return data[1:-1] #remove 'Start' and 'Stop' elements\n",
        "\n",
        "\n",
        "data = web_scrap()\n",
        "products_dataframe = pd.DataFrame(data = data, columns = data[0].keys())\n",
        "\n",
        "products_dataframe.to_csv(\"./result.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"result.csv\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 86.6 MB of archives.\n",
            "After this operation, 300 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 90.0.4430.72-0ubuntu0.18.04.1 [1,128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 90.0.4430.72-0ubuntu0.18.04.1 [76.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 90.0.4430.72-0ubuntu0.18.04.1 [3,858 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 90.0.4430.72-0ubuntu0.18.04.1 [4,743 kB]\n",
            "Fetched 86.6 MB in 4s (22.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_90.0.4430.72-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting webdriver-manager\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/01/f093567666448d7a9c4db85e0581ad5f3aa225af79e6eb71ae13b6bf20dc/webdriver_manager-3.4.1-py2.py3-none-any.whl\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting crayons\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/0d/e3fad4ca1de8e70e06444e7d777a5984261e1db98758b5be3e8296c03fe9/crayons-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (3.0.4)\n",
            "Installing collected packages: configparser, colorama, crayons, webdriver-manager\n",
            "Successfully installed colorama-0.4.4 configparser-5.0.2 crayons-0.4.0 webdriver-manager-3.4.1\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f2285548-d743-4fe6-832f-9eca8dedb88b\", \"result.csv\", 100879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}